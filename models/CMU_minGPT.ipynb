{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMU_minGPT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR9IfjWlf5FX",
        "outputId": "5349558c-3d7e-4116-ed60-c845770665b3"
      },
      "source": [
        "!git clone https://github.com/karpathy/mingpt.git\n",
        "%cd mingpt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mingpt'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Total 175 (delta 0), reused 0 (delta 0), pack-reused 175\u001b[K\n",
            "Receiving objects: 100% (175/175), 1.37 MiB | 5.87 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n",
            "/content/mingpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EHWpIS-R7h3",
        "outputId": "0ea0f7d7-21d5-4c9a-a53e-aaf28dffdd53"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrxKFrNnf047"
      },
      "source": [
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIPMeD2XgE0l"
      },
      "source": [
        "from mingpt.utils import set_seed\n",
        "set_seed(42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU6Sz4xogGtr"
      },
      "source": [
        "import urllib\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from collections import OrderedDict, Counter"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqUMnyz_hvw3",
        "outputId": "68f42e29-f5ea-4c5f-ade4-9eeaf51b0503"
      },
      "source": [
        "!gdown --id 1oAsRKCXzzzvhcElawXfjv1CBUo_Kr8g_\n",
        "!gdown --id 1YRW6j_Gp0Xxkad-fHNzyfECxssYAsmRE"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oAsRKCXzzzvhcElawXfjv1CBUo_Kr8g_\n",
            "To: /content/mingpt/cmudict-0.7b\n",
            "3.72MB [00:00, 118MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YRW6j_Gp0Xxkad-fHNzyfECxssYAsmRE\n",
            "To: /content/mingpt/cmudict.symbols\n",
            "100% 281/281 [00:00<00:00, 476kB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LbVavR9hyWD"
      },
      "source": [
        "IS_KAGGLE = False\n",
        "import os \n",
        "\n",
        "CMU_DICT_PATH = os.path.join(\n",
        "    '.', 'cmudict-0.7b')\n",
        "CMU_SYMBOLS_PATH = os.path.join(\n",
        "    '.','cmudict.symbols')\n",
        "\n",
        "# Skip words with numbers or symbols\n",
        "ILLEGAL_CHAR_REGEX = \"[^A-Z-'.]\"\n",
        "\n",
        "# Only 3 words are longer than 20 chars\n",
        "# Setting a limit now simplifies training our model later\n",
        "MAX_DICT_WORD_LEN = 20\n",
        "MIN_DICT_WORD_LEN = 2\n",
        "\n",
        "\n",
        "def load_clean_phonetic_dictionary():\n",
        "\n",
        "    def is_alternate_pho_spelling(word):\n",
        "        # No word has > 9 alternate pronounciations so this is safe\n",
        "        return word[-1] == ')' and word[-3] == '(' and word[-2].isdigit() \n",
        "\n",
        "    def should_skip(word):\n",
        "        if not word[0].isalpha():  # skip symbols\n",
        "            return True\n",
        "        if word[-1] == '.':  # skip abbreviations\n",
        "            return True\n",
        "        if re.search(ILLEGAL_CHAR_REGEX, word):\n",
        "            return True\n",
        "        if len(word) > MAX_DICT_WORD_LEN:\n",
        "            return True\n",
        "        if len(word) < MIN_DICT_WORD_LEN:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    phonetic_dict = {}\n",
        "    with open(CMU_DICT_PATH, encoding=\"ISO-8859-1\") as cmu_dict:\n",
        "        for line in cmu_dict:\n",
        "\n",
        "            # Skip commented lines\n",
        "            if line[0:3] == ';;;':\n",
        "                continue\n",
        "\n",
        "            word, phonetic = line.strip().split('  ')\n",
        "\n",
        "            # Alternate pronounciations are formatted: \"WORD(#)  F AH0 N EH1 T IH0 K\"\n",
        "            # We don't want to the \"(#)\" considered as part of the word\n",
        "            if is_alternate_pho_spelling(word):\n",
        "                word = word[:word.find('(')]\n",
        "\n",
        "            if should_skip(word):\n",
        "                continue\n",
        "\n",
        "            if word not in phonetic_dict:\n",
        "                phonetic_dict[word] = []\n",
        "            phonetic_dict[word].append(phonetic)\n",
        "\n",
        "    if IS_KAGGLE: # limit dataset to 5,000 words\n",
        "        phonetic_dict = {key:phonetic_dict[key] \n",
        "                         for key in random.sample(list(phonetic_dict.keys()), 5000)}\n",
        "    return phonetic_dict\n",
        "\n",
        "phonetic_dict = load_clean_phonetic_dictionary()\n",
        "example_count = np.sum([len(prons) for _, prons in phonetic_dict.items()])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnP-I8S1iTZD",
        "outputId": "33ddac85-19d6-4c1c-d077-8b4f05c9e286"
      },
      "source": [
        "phonetic_dict[' '] = ' '\n",
        "print(\"\\n\".join([k+' --> '+phonetic_dict[k][0] for k in random.sample(list(phonetic_dict.keys()), 10)]))\n",
        "print('\\nAfter cleaning, the dictionary contains %s words and %s pronunciations (%s are alternate pronunciations).' % \n",
        "      (len(phonetic_dict), example_count, (example_count-len(phonetic_dict))))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PEDIGREES --> P EH1 D AH0 G R IY0 Z\n",
            "BRUNCH --> B R AH1 N CH\n",
            "AMERIFIRST --> AH0 M EH1 R IH0 F ER0 S T\n",
            "SALE'S --> S EY1 L Z\n",
            "ESTHETICS --> EH0 S TH EH1 T IH0 K S\n",
            "DRIVES --> D R AY1 V Z\n",
            "DEXTROSE --> D EH1 K S T R OW0 S\n",
            "CELONA --> CH EH0 L OW1 N AH0\n",
            "RUNNELLS --> R AH1 N AH0 L Z\n",
            "BRANDTNER --> B R AE1 N T N ER0\n",
            "\n",
            "After cleaning, the dictionary contains 124815 words and 133569 pronunciations (8754 are alternate pronunciations).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLYAxMsvibA1"
      },
      "source": [
        "data = ''.join(x.lower() + '\\n' for x in phonetic_dict)\n",
        "targets = ''.join(re.sub('\\d', '',phonetic_dict[x][0]) + '\\n' for x in phonetic_dict)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbdWsF1ZgLIq"
      },
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, data, vocab_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = self.build_vocab(data)\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(self.vocab) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(self.vocab) }\n",
        "    \n",
        "    def sort_vocab(self, vocab):\n",
        "        \"\"\"\n",
        "        Vocab should have the followind order: hashtag, numbers, characters sorted by length.\n",
        "        Hashtags should go first, because they will be used as dividers on tokenization step.\n",
        "        Numbers should go before characters, because token ids are numbers. Otherwise token ids will be considered as usual numbers and replaced twice.\n",
        "        \"\"\"\n",
        "        sorted_vocab = sorted(vocab, key=lambda x: len(x), reverse=True)\n",
        "        tag = [int(s) for s in sorted_vocab if s == '#']\n",
        "        \n",
        "        numeric = [int(s) for s in sorted_vocab if s.isnumeric()]\n",
        "        numeric = [str(s) for s in sorted(numeric, reverse=True)]\n",
        "        rest = [s for s in sorted_vocab if not s.isnumeric()]\n",
        "        \n",
        "        sorted_vocab = tag + numeric + rest\n",
        "        \n",
        "        return sorted_vocab\n",
        "    \n",
        "    def build_vocab(self, data):\n",
        "        \"\"\"\n",
        "        Build vocabluary using BPE alghorithm.\n",
        "        \"\"\"\n",
        "        vocab = set(data)\n",
        "        if len(vocab) > self.vocab_size:\n",
        "            raise ValueError('Vocab size should be greater than unique char count')\n",
        "\n",
        "        # check all available characters\n",
        "        char_set = {c for c in vocab if c.isalpha()}\n",
        "        \n",
        "        # candidates dictionary will contain a set of all available tokens to search\n",
        "        candidate_dict = dict().fromkeys(char_set, 0)\n",
        "        \n",
        "        # occurrences will contain all matched tokens and the count, how many times the token has been found.\n",
        "        token_occurrences = OrderedDict()\n",
        "        while len(vocab) < self.vocab_size:\n",
        "            for candidate in candidate_dict.keys():\n",
        "                occurrences = data.count(candidate)\n",
        "                candidate_dict[candidate] = occurrences\n",
        "\n",
        "            candidate_dict = {candidate: count for candidate, count in candidate_dict.items() if count}\n",
        "            vocab.update(set(candidate_dict.keys()))\n",
        "            token_occurrences.update(candidate_dict)\n",
        "\n",
        "            # build new candidates\n",
        "            temp_candidate_set = set()\n",
        "            for char in char_set:\n",
        "                # don't test candidates with occurency <= 2. New candidates won't have occurency higher than 2\n",
        "                temp_candidate_set.update({candidate + char for candidate in candidate_dict.keys() if token_occurrences[candidate] > 2})\n",
        "\n",
        "            candidate_dict = dict().fromkeys(temp_candidate_set, 0)\n",
        "\n",
        "        tokens_to_remove = len(vocab) - self.vocab_size\n",
        "        token_occurrences = OrderedDict(sorted(token_occurrences.items(), key=lambda x: x[1], reverse=True))\n",
        "        for _ in range(tokens_to_remove):\n",
        "            token, _ = token_occurrences.popitem()\n",
        "            vocab.remove(token)\n",
        "\n",
        "        sorted_vocab = self.sort_vocab(vocab)\n",
        "        \n",
        "        # add a special token for unknown tokens\n",
        "        sorted_vocab.append('<unk>')\n",
        "        self.vocab_size += 1 # plus <unk> special token\n",
        "        \n",
        "        return sorted_vocab\n",
        "    \n",
        "    def tokenize(self, data, block_size):\n",
        "        for token in self.vocab:\n",
        "            data = data.replace(token, f'#{self.stoi[token]}#')\n",
        "\n",
        "        # If everything went well, first and last characters won't have # pair. Need to trim them\n",
        "        data = data[1:-1]\n",
        "        # Split by ## pairs\n",
        "        tokenized_text = data.split('##')\n",
        "        # Filter empty strings\n",
        "        tokenized_text = [x for x in tokenized_text if x]\n",
        "        result = []\n",
        "        for tokenized in tokenized_text:\n",
        "            # In case other single # found, replace them with <unk> special token, marking the element as unknown\n",
        "            if '#' in tokenized:\n",
        "                for unknown_candidate in tokenized.split('#'):\n",
        "                    if unknown_candidate.isnumeric():\n",
        "                        result.append(self.itos[int(unknown_candidate)])\n",
        "                    else:\n",
        "                        result.append('<unk>')\n",
        "            else:\n",
        "                result.append(self.itos[int(tokenized)])\n",
        "\n",
        "        # all texts should have equal size. We can make text length equal by filling text with spaces\n",
        "        for _ in range(block_size - len(result)):\n",
        "            result.append(' ')\n",
        "            \n",
        "        # in case the sentence is longer, than block_size, we trim the sentence\n",
        "        return result[:block_size]\n",
        "    \n",
        "    def encode(self, data):\n",
        "        return [self.stoi[s] for s in data]\n",
        "    \n",
        "    def decode(self, data):\n",
        "        return ''.join([self.itos[int(i)] for i in data])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHKnyD8lgVcK"
      },
      "source": [
        "vocab_size = 49\n",
        "# building vocabluary can take some time. ~5 minutes for 10_000 tokens for each tokenizer. \n",
        "tokenizer_data = Tokenizer(data, vocab_size)\n",
        "tokenizer_targets = Tokenizer(targets, vocab_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78e4rN9wW_iE",
        "outputId": "66626217-649f-4a1a-f2d1-8e06c5016a94"
      },
      "source": [
        "tokenizer_data.encode(\" \")l,"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[46]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bTHc-jPV54m"
      },
      "source": [
        "torch.save(tokenizer_data,'../tok_data.pt')\n",
        "torch.save(tokenizer_targets,'../tok_tar.pt')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgsrdwnxV6AO"
      },
      "source": [
        "tokenizer_data = torch.load('../tok_data.pt')\n",
        "tokenizer_targets = torch.load('../tok_tar.pt')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwVr6NUlWyZU",
        "outputId": "8fd64e17-772d-40e2-d331-fb582e923f5b"
      },
      "source": [
        "tokenizer_data.encode(\" \")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[46]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H193gUTggXKm"
      },
      "source": [
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, original, modern, tokenizer_targets, tokenizer_data, block_size):\n",
        "        self.tokenizer_targets = tokenizer_targets\n",
        "        self.tokenizer_data = tokenizer_data\n",
        "        \n",
        "        self.block_size = block_size * 2\n",
        "        self.original = [tokenizer_targets.tokenize(t, block_size) for t in original]\n",
        "        self.modern = [tokenizer_data.tokenize(t, block_size) for t in modern]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.original)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        The idea is to get a sentence in a modern English\n",
        "        and translate it to Shakespeare English.\n",
        "        \n",
        "        In the init method we already split a sentence into tokens and filled with spaces,\n",
        "        to have an equal sentence size. In this method we just encode the tokens to\n",
        "        ids (a list of numbers), and we're trying to map ids sequences\n",
        "        (original Englisn and modern English)\n",
        "        \"\"\"\n",
        "        \n",
        "        modern_text = self.tokenizer_data.encode(self.modern[idx])\n",
        "        original_text = self.tokenizer_targets.encode(self.original[idx])\n",
        "        dix = modern_text + original_text\n",
        "        \n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        y[:int(self.block_size / 2) - 1] = -100\n",
        "        \n",
        "        return x, y"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adNB11cLgeQW"
      },
      "source": [
        "\n",
        "# Shuffle texts by lines\n",
        "texts = list(zip(data.splitlines(), targets.splitlines()))\n",
        "random.shuffle(texts)\n",
        "\n",
        "data, targets = zip(*texts)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhtdVBFbgiWU"
      },
      "source": [
        "\n",
        "# Split texts into train, test and validation datasets\n",
        "train_dataset_size = round(0.9 * len(data))\n",
        "test_dataset_size = round(0.1 * len(data))\n",
        "\n",
        "train_data = data[:train_dataset_size]\n",
        "test_modern = data[train_dataset_size:train_dataset_size + test_dataset_size]\n",
        "\n",
        "train_targets = targets[:train_dataset_size]\n",
        "test_original = targets[train_dataset_size:train_dataset_size + test_dataset_size]\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTeM4lo2gtnx"
      },
      "source": [
        "\n",
        "block_size = 100  # the estimate how long lines the text could be (token count)\n",
        "\n",
        "train_dataset = Dataset(train_targets, train_data, tokenizer_targets, tokenizer_data, block_size)\n",
        "test_dataset = Dataset(test_original, test_modern, tokenizer_targets, tokenizer_data, block_size)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJTsQ7Yg1C9",
        "outputId": "497af44f-88f4-407f-bb95-74a3d1fa6f66"
      },
      "source": [
        "from mingpt.model import GPT, GPTConfig\n",
        "mconf = GPTConfig(tokenizer_targets.vocab_size, train_dataset.block_size,\n",
        "                  n_layer=2, n_head=4, n_embd=512)\n",
        "model = GPT(mconf)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/02/2021 13:55:05 - INFO - mingpt.model -   number of parameters: 6.459392e+06\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FMN7_0Eg3ij"
      },
      "source": [
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "tokens_per_epoch = len(train_dataset) * block_size\n",
        "train_epochs = 20\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "tconf = TrainerConfig(max_epochs=train_epochs, batch_size=64, learning_rate=3e-4,\n",
        "                      lr_decay=True, warmup_tokens=tokens_per_epoch, final_tokens=train_epochs*tokens_per_epoch,\n",
        "                      num_workers=2)\n",
        "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L94sDssRFok"
      },
      "source": [
        "torch.save(model.state_dict(),'../model.pt')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aosVik1cQ6Og"
      },
      "source": [
        "def predict(context):\n",
        "  x = torch.tensor(tokenizer_data.encode(tokenizer_data.tokenize(context, block_size)), dtype=torch.long)[None,...].to(trainer.device)\n",
        "  y = sample(model, x, block_size, temperature=1.0, sample=True, top_k=10)[0]\n",
        "\n",
        "  predicted = y[block_size:]\n",
        "  return tokenizer_targets.decode(predicted)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PTbCry2g5AX",
        "outputId": "bf9c3759-b780-4ac0-ef97-c82298241767"
      },
      "source": [
        "from mingpt.utils import sample\n",
        "from random import choice\n",
        "model.to('cuda:0')\n",
        "for _ in range(5):\n",
        "    idx = choice(range(len(test_original)))\n",
        "\n",
        "    context = test_modern[idx]\n",
        "    \n",
        "    print(f'Word:                    {context}')\n",
        "    print(f'Predicted pronouncation: {predict(context)}')\n",
        "    print(f'Real pronunciation:      {test_original[idx]}')\n",
        "    print('--------------------------------------------------')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word:                    consumers'\n",
            "Predicted pronouncation: K AH N S UW M ER Z                                                                                     \n",
            "Real pronunciation:      K AH N S UW M ER Z\n",
            "--------------------------------------------------\n",
            "Word:                    zsa\n",
            "Predicted pronouncation: SH AA                                                                                                 \n",
            "Real pronunciation:      ZH AA\n",
            "--------------------------------------------------\n",
            "Word:                    backbiting\n",
            "Predicted pronouncation: B AE K B AY T IH NG                                                                                     \n",
            "Real pronunciation:      B AE K B AY T IH NG\n",
            "--------------------------------------------------\n",
            "Word:                    gurion\n",
            "Predicted pronouncation: G Y UH R IY AH N                                                                                       \n",
            "Real pronunciation:      G Y UH R IY AH N\n",
            "--------------------------------------------------\n",
            "Word:                    mathematical\n",
            "Predicted pronouncation: M AE TH AH M AE T IH K AH L                                                                               \n",
            "Real pronunciation:      M AE TH AH M AE T IH K AH L\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlnxi6_7hfBF",
        "outputId": "86d921ce-4bfe-4030-c7b3-179f5de6d93c"
      },
      "source": [
        "print(predict('aditya'))\n",
        "print(predict('vishal'))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AH D IH T Y AH                                                                                         \n",
            "V IH SH AH L                                                                                           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqmG3o1YlrD-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}